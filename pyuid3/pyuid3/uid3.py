# AUTOGENERATED! DO NOT EDIT! File to edit: src/uId3.ipynb (unless otherwise specified).

__all__ = ['UId3']

# Cell
from graphviz import Source
from sklearn.base import BaseEstimator
import numpy as np
import pandas as pd

from .attribute import Attribute
from .data import Data
from .entropy_evaluator import EntropyEvaluator
from .tree import Tree
from .tree_node import TreeNode
from .tree_edge import TreeEdge
from .tree_evaluator import TreeEvaluator
from .value import Value
from .reading import Reading
from .instance import Instance
from .utils import StandardRescaler
from multiprocessing import cpu_count,Pool
import math
import shap
from sklearn.svm import LinearSVC
from sklearn.preprocessing import StandardScaler

from imblearn.over_sampling._smote.base import BaseSMOTE
import numpy as np 

import math
import numbers
import warnings
from collections import Counter

from scipy import sparse
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.utils import _safe_indexing, check_array, check_random_state


from imblearn.utils import Substitution, check_target_type
from imblearn.utils._docstring import _n_jobs_docstring, _random_state_docstring


class UncertainSMOTEID3(BaseSMOTE):
        
    def __init__(
        self,
        *,
        predict_proba,
        sampling_strategy="all",
        random_state=None,
        k_neighbors=5,
        n_jobs=None,
        sigma=1,
        m_neighbors=10,
        min_samples=300,
        kind="borderline-1",
    ):
        super().__init__(
            sampling_strategy=sampling_strategy,
            random_state=random_state,
            k_neighbors=k_neighbors,
            n_jobs=n_jobs,
        )
        self.m_neighbors = m_neighbors
        self.kind = kind
        self.predict_proba=predict_proba
        self.sigma=sigma
        self.min_samples=min_samples


    def _fit_resample(self, X, y):
        # FIXME: to be removed in 0.12
        if self.n_jobs is not None:
            warnings.warn(
                "The parameter `n_jobs` has been deprecated in 0.10 and will be "
                "removed in 0.12. You can pass an nearest neighbors estimator where "
                "`n_jobs` is already set instead.",
                FutureWarning,
            )

        self._validate_estimator()

        X_resampled = X.copy()
        y_resampled = y.copy()

        #n_samples can be passed as an argument, n_iterations can also be used here
        for class_sample, n_samples in self.sampling_strategy_.items():
            no_class_samples = len(X_resampled[y_resampled ==class_sample])
            if no_class_samples < self.min_samples:
                n_samples = self.min_samples-no_class_samples
            elif n_samples == 0:
                continue
            target_class_indices = np.flatnonzero(y == class_sample)
            X_class = _safe_indexing(X, target_class_indices)

            #self.nn_m_.fit(X)
            danger_index = self._in_danger_noise(
                #self.nn_m_, 
                self.predict_proba,
                X_class, class_sample, y, kind="danger"
            )
            if not any(danger_index):
                continue

            self.nn_k_.fit(X_class)
            nns = self.nn_k_.kneighbors(
                _safe_indexing(X_class, danger_index), return_distance=False
            )[:, 1:]

            # divergence between borderline-1 and borderline-2
            if self.kind == "borderline-1":
                # Create synthetic samples for borderline points.
                X_new, y_new = self._make_samples(
                    _safe_indexing(X_class, danger_index),
                    y.dtype,
                    class_sample,
                    X_class,
                    nns,
                    n_samples,
                )
                if sparse.issparse(X_new):
                    X_resampled = sparse.vstack([X_resampled, X_new])
                else:
                    X_resampled = np.vstack((X_resampled, X_new))
                y_resampled = np.hstack((y_resampled, y_new))

            elif self.kind == "borderline-2":
                random_state = check_random_state(self.random_state)
                fractions = random_state.beta(10, 10)

                # only minority
                X_new_1, y_new_1 = self._make_samples(
                    _safe_indexing(X_class, danger_index),
                    y.dtype,
                    class_sample,
                    X_class,
                    nns,
                    int(fractions * (n_samples + 1)),
                    step_size=1.0,
                )

                # we use a one-vs-rest policy to handle the multiclass in which
                # new samples will be created considering not only the majority
                # class but all over classes.
                X_new_2, y_new_2 = self._make_samples(
                    _safe_indexing(X_class, danger_index),
                    y.dtype,
                    class_sample,
                    _safe_indexing(X, np.flatnonzero(y != class_sample)),
                    nns,
                    int((1 - fractions) * n_samples),
                    step_size=0.5,
                )

                if sparse.issparse(X_resampled):
                    X_resampled = sparse.vstack([X_resampled, X_new_1, X_new_2])
                else:
                    X_resampled = np.vstack((X_resampled, X_new_1, X_new_2))
                y_resampled = np.hstack((y_resampled, y_new_1, y_new_2))

        return X_resampled, y_resampled
    
    
    
    def _in_danger_noise(self, predict_proba, samples, target_class, y, kind="danger"):
        """Estimate if a set of sample are in danger or noise.
        Used by BorderlineSMOTE and SVMSMOTE.
        Parameters
        ----------
        nn_estimator : estimator object
            An estimator that inherits from
            :class:`~sklearn.neighbors.base.KNeighborsMixin` use to determine
            if a sample is in danger/noise.
            NOT USED
        samples : {array-like, sparse matrix} of shape (n_samples, n_features)
            The samples to check if either they are in danger or not.
        target_class : int or str
            The target corresponding class being over-sampled.
        y : array-like of shape (n_samples,)
            The true label in order to check the neighbour labels.
            NOT USED
        kind : {'danger', 'noise'}, default='danger'
            The type of classification to use. Can be either:
            - If 'danger', check if samples are in danger,
            - If 'noise', check if samples are noise.
        Returns
        -------
        output : ndarray of shape (n_samples,)
            A boolean array where True refer to samples in danger or noise.
        """
        
        c_labels  = samples[np.argmax(self.predict_proba(samples),axis=1) ==target_class]
        prediction_certainty = np.max(self.predict_proba(c_labels),axis=1)
        
        #shuld this be thresholded like that, or keep n-lowest?
        confidence_threshold = np.mean(prediction_certainty)-self.sigma*np.std(prediction_certainty)
        
        
        #x = nn_estimator.kneighbors(samples, return_distance=False)[:, 1:]
        
#         nn_label = (y[x] != target_class).astype(int)
#         n_maj = np.sum(nn_label, axis=1)

        if kind == "danger":
#             # Samples are in danger for m/2 <= m' < m
#             return np.bitwise_and(
#                 n_maj >= (nn_estimator.n_neighbors - 1) / 2,
#                 n_maj < nn_estimator.n_neighbors - 1,
#             )
            return prediction_certainty<confidence_threshold
        else:  # kind == "noise":
#             # Samples are noise for m = m'
#             return n_maj == nn_estimator.n_neighbors - 1
            return prediction_certainty<0 #always false
        
        
        
       
        
        
    

# Cell
class UId3(BaseEstimator):
    
    PARALLEL_ENTRY_FACTOR = 1000

    def __init__(self, max_depth=2, node_size_limit = 1, grow_confidence_threshold = 0, min_impurity_decrease=0):
        self.TREE_DEPTH_LIMIT= max_depth
        self.max_depth = max_depth
        self.node_size_limit=node_size_limit
        self.NODE_SIZE_LIMIT = node_size_limit
        self.grow_confidence_threshold = grow_confidence_threshold
        self.tree = None
        self.node_size_limit = node_size_limit
        self.min_impurity_decrease=min_impurity_decrease
        
    @staticmethod
    def generate_uarff(X,y,class_names,X_importances=None, categorical = None):
        """ Generates uncertain ARFF file
        Arguments:
            X : DataFrame containing dataset for training
            y : target values returned by predict_proba function
            class_names : names for the classess to be used in uID3
            X_confidence : confidence for each reading obtained. This matrix should be normalized to the range [0;1].
        
        """
        if X_importances is not None:
            if not isinstance(X_importances, pd.DataFrame):
                raise ValueError('Reading confidence matrix has to be DataFrame.')
            if X.shape != X_importances.shape:
                raise ValueError("Confidence for readings have to be exaclty the size of X.")
        if categorical is None:
            categorical = [False]*X.shape[1]
                                 
        uarff="@relation lux\n\n"
        for i,(f,t) in enumerate(zip(X.columns,X.dtypes)):
            if t in (int, float, np.int32, np.int64, np.int) and not categorical[i]:
                uarff+=f'@attribute {f} @REAL\n'
            elif categorical[i]:
                domain = ','.join(map(str,list(X[f].unique())))
                uarff+='@attribute '+f+' {'+domain+'}\n'

        domain = ','.join([str(cn) for cn in class_names])
        uarff+='@attribute class {'+domain+'}\n'
        
        uarff += '@data\n'
        for i in range(0, X.shape[0]):
            for j in range(0,X.shape[1]):
                if X_importances is not None:
                    uarff+=f'{X.iloc[i,j]}[{X_importances.iloc[i,j]}],'
                else:
                    uarff+=f'{X.iloc[i,j]}[1],'
            uarff+=';'.join([f'{c}[{p}]' for c,p in zip(class_names, y[i,:])])+'\n'
        return uarff
        
    def fit(self, data, y=None, *, depth,  entropyEvaluator, classifier=None,  beta=1, discount_importance = False, prune=False, oblique=False,  n_jobs=None,imp=None,logo='',bid='r'): 
        """Fits pyUID3 tree, optionally using SHAP values calculated for the classifier.

        Parameters
        ----------
        data : pyuid3.Data
            Data object containing dataset. It has to be object from pyuid3.Data
        y : np.array
            Vector containing target values
        depth : int, optional
            This parameter should not be used. It is used internally by recurrent calls to govern the depth of the tree.
        entropyEvaluator: pyuid3.EntropyEvaluator
            Object responisble for calculating split criterion. Default is UncertainEntropyEvaluator. Although the naming might be confusing, other possibilities are:
            UncertainGiniEvaluator, UncertainSqrtGiniEvaluator
        classifier: optional
            A classifier that is designed according to sckit paradigm. It is required from the classifier to have predict_proba function. Default is None
        beta: int
            Parameter being a weight in harmonic mean between score obtained from EntropyEvaluator and SHAP values. 
            The greater the value the more important are SHAP values when selecting a split. Default is 1.
        discount_importance: boolean,
            Parameter indicating if the SHAP importances should be calculated resively at every split, or if the importances calculated for the whole data should be used.
            In the latter case, the importances are discounted by the percentage of reduction in split criterion (e.g. Information Gain). Default it False.
        prune: boolean, optional
            Define if after training the tree should be pruned. The prounning is done by looking at the change in prediction on a training set. If removing a branch does not change the prediction outcome, the branch is pruned. It will provide more general trees, i.e.rules extracted from branches will have more coverage, but their precission may drop.
        oblique: boolean, optional
            Define if the tree should assume building linear slipts, instead of simple inequality-based spolits. Deafult False.
        n_jobs: int, optional
            Number of processess to use when building a tree. Default is None
        

        Returns
        -------
        pyuid3.Tree
            a fitted decision tree
        """
        log=''
        if imp is not None:
            shap_values=[sv for sv in np.moveaxis(imp, 2,0)]
            expected_values=[np.mean(v) for v in shap_values]
        elif classifier is not None and len(data.get_instances()) >= self.NODE_SIZE_LIMIT:
            datadf = data.to_dataframe()     
            # try:
            #     datadfx = datadf.iloc[:,:-1]
            #     sm = UncertainSMOTEID3(predict_proba=classifier.predict_proba,sigma=1,sampling_strategy='all') 
            #     datadfx, _ = sm.fit_resample(datadfx, np.argmax(classifier.predict_proba(datadfx),axis=1))
            #     y_train_sample = classifier.predict_proba(datadfx)
            #     #limit features here
            #     uarff=UId3.generate_uarff(datadfx,y_train_sample, X_importances=None,categorical=None,class_names=[0,1])
            #     data = Data.parse_uarff_from_string(uarff)
            #     datadf = data.to_dataframe()
            # except:
            #     pass
            
            try:
                bgmaxsize = datadf.shape[0]
                bg = datadf.iloc[:,:-1].sample(min(bgmaxsize, 500))
                explainer = shap.Explainer(classifier,bg)#datadf.iloc[:,:-1])
                if hasattr(explainer, "shap_values"):
                    shap_values = explainer.shap_values(datadf.iloc[:,:-1],check_additivity=False)
                else:
                    shap_values = explainer(datadf.iloc[:,:-1]).values
                    shap_values=[sv for sv in np.moveaxis(shap_values, 2,0)]
                if hasattr(explainer, "expected_value"):
                    expected_values = explainer.expected_value
                else:
                    expected_values=[np.mean(v) for v in shap_values]
            except TypeError:
                explainer = shap.Explainer(classifier.predict_proba, bg)#datadf.iloc[:,:-1])
                shap_values = explainer(datadf.iloc[:,:-1]).values
                shap_values=[sv for sv in np.moveaxis(shap_values, 2,0)]
                expected_values=[np.mean(v) for v in shap_values]
            
            
            if type(shap_values) is not list:
                shap_values = [-shap_values, shap_values]
                expected_values=[np.mean(v) for v in shap_values]

            #find max and rescale:
            maxshap = max([np.max(np.abs(sv)) for sv in shap_values]) #ADD
            shap_values = [sv/maxshap for sv in shap_values] #ADD
            #print(f'Max {maxshap}')
            #print(f'all {shap_values}')
            
            shap_dict={}
            expected_dict={}
            for i,v in enumerate(shap_values):
                shap_dict[str(i)] = pd.DataFrame(v, columns = datadf.columns[:-1])
                expected_dict[str(i)] = expected_values[i] /maxshap #ADD
                
            data = data.set_importances(pd.concat(shap_dict,axis=1), expected_values = expected_dict)
        
        if len(data.get_instances()) < self.NODE_SIZE_LIMIT:
            return None,log
        if depth > self.TREE_DEPTH_LIMIT:
            return None,log
        entropy = entropyEvaluator.calculate_entropy(data)
        
        data.update_attribute_domains()

        # of the set is heterogeneous or no attributes to split, just class -- return
        # leaf
        if entropy == 0 or len(data.get_attributes()) == 1:
            # create the only node and summary for it
            class_att = data.get_class_attribute()
            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))
            root.set_type(class_att.get_type())
            tree = Tree(root)
            if depth == 0:
                self.tree = tree
            return tree,log

        info_gain = 0
        best_split = None
        
        cl=[]
        for i in data.get_instances():
            cl.append(i.get_reading_for_attribute(data.get_class_attribute()).get_most_probable().get_name())

        n_jobs_inner = 1
        if n_jobs is not None:
            if n_jobs == -1:
                n_jobs=n_jobs_inner = cpu_count()
            if len(data)/n_jobs_inner < UId3.PARALLEL_ENTRY_FACTOR:
                n_jobs_inner = max(1,int(len(data)/UId3.PARALLEL_ENTRY_FACTOR)) 
            if n_jobs > len(data.get_attributes()):
                n_jobs = len(data.get_attributes())-1
            if (len(data)*len(data.get_attributes()))/n_jobs < UId3.PARALLEL_ENTRY_FACTOR:
                n_jobs = max(1,int((len(data)*len(data.get_attributes()))/UId3.PARALLEL_ENTRY_FACTOR))
        else:
            n_jobs = 1
            
        gains = []
        if n_jobs > 1 and n_jobs_inner < len(data.get_attributes()):
            with Pool(n_jobs) as pool:
                print(entropy)
                results = pool.starmap(UId3.try_attribute_for_split, [(data, a, cl, entropy, entropyEvaluator,self.min_impurity_decrease, beta, 1,classifier is not None) for a in data.get_attributes() if a != data.get_class_attribute()])
                temp_gain = 0
                for temp_gain, pure_temp_gain, best_split_candidate in results:
                    if best_split_candidate is not None:
                        gains.append((temp_gain,pure_temp_gain,best_split_candidate))
                    if temp_gain > info_gain and (pure_temp_gain/entropy)>=self.min_impurity_decrease:
                        info_gain = temp_gain
                        pure_info_gain=pure_temp_gain
                        best_split = best_split_candidate
        else:
            for a in data.get_attributes():
                if data.get_class_attribute() == a:
                    continue
                temp_gain, pure_temp_gain, best_split_candidate=self.try_attribute_for_split(data, a, cl, entropy, entropyEvaluator,self.min_impurity_decrease, beta=beta, n_jobs=n_jobs, shap = classifier is not None)
                ss = data.calculate_statistics(a)
                cls = (np.array(cl)=='1')
                log += f'{logo},{depth},{bid}{depth},{sum(cls)},{len(cl)-sum(cls)},{a.get_name()},{pure_temp_gain},{ss.get_avg_abs_importance()}\n'
                if best_split_candidate is not None:
                    gains.append((temp_gain,pure_temp_gain,best_split_candidate))
                if temp_gain > info_gain and (pure_temp_gain/entropy)>=self.min_impurity_decrease:
                    info_gain = temp_gain
                    pure_info_gain=pure_temp_gain
                    best_split = best_split_candidate

        ###########################################
        #if there is a shap
        try:
            if oblique:
                svm_temp_gain = pure_svm_temp_gain = 0
                if classifier is not None:
                    #select two most important features according to SHAP
                    ivmean = data.to_dataframe_importances(average_absolute=True)
                    idd = np.flip(np.argsort(ivmean)[-2:])
                    features = [f for f in data.get_attributes() if f not in [data.get_class_attribute().get_name()]]
                    svc_features = [features[i] for i in idd]
                    svm_temp_gain, pure_svm_temp_gain, svm_best_splitting_att,svm_best_linear_att, boundary_expression = UId3.get_oblique_gains(data, svc_features,entropyEvaluator, entropy, beta, shap=True)
       
                elif len(gains) > 1:
                    #take two most importnat selected by Dtree
                    gains = sorted(gains,key=lambda x: x[0],reverse=True)
                    svc_features = [gains[0][2].get_name(),gains[1][2].get_name()]
                    svm_temp_gain, pure_svm_temp_gain, svm_best_splitting_att, svm_best_linear_att, boundary_expression = UId3.get_oblique_gains(data, svc_features,entropyEvaluator, entropy, beta, shap=False)
    
            
                if svm_temp_gain > info_gain and (pure_svm_temp_gain/entropy)>=self.min_impurity_decrease:
                    info_gain = svm_temp_gain
                    pure_info_gain=pure_svm_temp_gain
                    best_split = svm_best_splitting_att
                    best_split.set_value_to_split_on(boundary_expression)
        except:
            pass
        
    
        ###########################################
        
        # if nothing better can happen
        if best_split == None:
            # create the only node and summary for it
            class_att = data.get_class_attribute()
            root = TreeNode(class_att.get_name(), data.calculate_statistics(class_att))
            root.set_type(class_att.get_type())
            tree = Tree(root)
            if depth == 0:
                self.tree = tree
            return tree,log

        # Create root node, and recursively go deeper into the tree.
        class_att = data.get_class_attribute()
        class_stats = data.calculate_statistics(class_att)
        root = TreeNode(best_split.get_name(), class_stats)
        root.set_type(class_att.get_type())
        
        classes = []
        # attach newly created trees
        for val in best_split.get_splittable_domain():
            if best_split.get_type() == Attribute.TYPE_NOMINAL:
                best_split_stats = data.calculate_statistics(best_split)
                new_data = data.filter_nominal_attribute_value(best_split, val)
                if not discount_importance:
                    subtree,logs = self.fit(new_data, classifier=classifier, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, prune=prune, oblique=oblique,n_jobs=n_jobs,logo=logo,bid=val)
                    log+=logs
                else:
                    if oblique and svm_temp_gain > 0:
                        new_data = new_data.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy/2.0)
                        new_data = new_data.reduce_importance_for_attribute(svm_best_linear_att, best_split.get_importance_gain()/entropy/2.0)
                    else:
                        new_data = new_data.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)
                    
                    subtree,logs = self.fit(new_data, discount_importance=True, classifier=None, entropyEvaluator=entropyEvaluator, depth=depth + 1,beta=beta, prune=prune,oblique=oblique, n_jobs=n_jobs,logo=logo,bid=val)
                    log+=logs
                if subtree and best_split_stats.get_most_probable().get_confidence() > self.grow_confidence_threshold:
                    if subtree.get_root().is_leaf():
                        classes.append(subtree.get_root().get_stats().get_most_probable().get_name())
                    root.add_edge(TreeEdge(Value(val, best_split_stats.get_avg_confidence()), subtree.get_root()))
                    root.set_infogain(best_split.get_importance_gain())

            elif best_split.get_type() == Attribute.TYPE_NUMERICAL:
                best_split_stats = data.calculate_statistics(best_split)
                new_data_less_then,new_data_greater_equal = data.filter_numeric_attribute_value_expr(best_split, val)
                
                
                if len(new_data_less_then) >= self.node_size_limit and len(new_data_greater_equal) >= self.node_size_limit:
                    if not discount_importance:
                        subtree_less_than,logs1 = self.fit(new_data_less_then, classifier=classifier, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, prune=prune, oblique=oblique,n_jobs=n_jobs,logo=logo,bid='lt')
                        subtree_greater_equal,logs2 = self.fit(new_data_greater_equal, classifier=classifier, entropyEvaluator=entropyEvaluator, depth=depth + 1, beta=beta, prune=prune,oblique=oblique, n_jobs=n_jobs,logo=logo,bid='gte')
                        log+=logs1+logs2
                    else:
                        if oblique and svm_temp_gain > 0:
                            new_data_less_then = new_data_less_then.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy/2.0)
                            new_data_greater_equal = new_data_greater_equal.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy/2.0)
                            
                            new_data_less_then = new_data_less_then.reduce_importance_for_attribute(svm_best_linear_att, best_split.get_importance_gain()/entropy/2.0)
                            new_data_greater_equal = new_data_greater_equal.reduce_importance_for_attribute(svm_best_linear_att, best_split.get_importance_gain()/entropy/2.0)
                        else:
                            new_data_less_then = new_data_less_then.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)
                            new_data_greater_equal = new_data_greater_equal.reduce_importance_for_attribute(best_split, best_split.get_importance_gain()/entropy)
                        
                        
                        subtree_less_than,logs1 = self.fit(new_data_less_then, classifier=None,  entropyEvaluator=entropyEvaluator, depth=depth + 1, discount_importance=True,beta=beta, prune=prune,oblique=oblique, n_jobs=n_jobs,logo=logo,bid='lt')
                        subtree_greater_equal,logs2 = self.fit(new_data_greater_equal, classifier=None, entropyEvaluator=entropyEvaluator, depth=depth + 1, discount_importance=True,beta=beta, prune=prune, oblique=oblique,n_jobs=n_jobs,logo=logo,bid='gte')
                        log+=logs1+logs2
                    if subtree_less_than and best_split_stats.get_most_probable().get_confidence() > self.grow_confidence_threshold:
                        root.add_edge(TreeEdge(Value("<" + val, best_split_stats.get_avg_confidence()), subtree_less_than.get_root()))
                        if subtree_less_than.get_root().is_leaf():
                            classes.append(subtree_less_than.get_root().get_stats().get_most_probable().get_name())
                    if subtree_greater_equal and best_split_stats.get_most_probable().get_confidence() > self.grow_confidence_threshold:
                        root.add_edge(TreeEdge(Value(">=" + val, best_split_stats.get_avg_confidence()), subtree_greater_equal.get_root()))
                        if subtree_greater_equal.get_root().is_leaf():
                            classes.append(subtree_greater_equal.get_root().get_stats().get_most_probable().get_name())
                    root.set_type(Attribute.TYPE_NUMERICAL)
                    root.set_infogain(best_split.get_importance_gain())

        #If all of the leaves predict same class, simply remove them, when prune is True
        if prune and len(classes) == len(root.get_edges()) and len(set(classes)) < 2:
            root.set_edges([])
        
        if len(root.get_edges()) == 0:
            root.set_att(data.get_class_attribute().get_name())
            root.set_type(data.get_class_attribute().get_type())

        self.tree = Tree(root)
        return self.tree,log

    @staticmethod
    def get_oblique_gains(data, svc_features,entropyEvaluator, globalEntropy,beta, shap):
        svc = LinearSVC()
        datadf = data.to_dataframe()
        if datadf[data.get_class_attribute().get_name()].nunique() < 2:
            return 0, 0, None, None, None
        
        sc = StandardScaler()
        sc.fit(datadf[svc_features])
        datadf.loc[:,svc_features] = sc.transform(datadf.loc[:,svc_features])
        svc.fit(datadf[svc_features], datadf[data.get_class_attribute().get_name()])     

    
        sr = StandardRescaler(sc.mean_, sc.scale_) 
        single_temp_gain_max = 0
        pure_single_temp_gain_max = 0
        boundary_expression=None
        boundary_expression_max=None
        for ci in range(0,len(svc.coef_)):
            coefs = svc.coef_[ci]
            intercept= svc.intercept_[ci]
            coefs, intercept = sr.rescale(coefs, intercept)
            #transform to canonical form

            sign =  np.sign(coefs[0])
            intercept /= coefs[0] 
            coefs /= coefs[0] 

            #moving to the other side of equation
            coefs[1:] = -1.0*coefs[1:]
            intercept *= -1

            boundary_expression = '+'.join([f'{c} * {f}' for c,f in  zip(coefs[1:], svc_features[1:])])+f'+{intercept}'
            splitting_att = data.get_attribute_of_name(svc_features[0])
            linear_relation_att = data.get_attribute_of_name(svc_features[1])
            if sign < 0:
                subdata_less_than,subdata_greater_equal = data.filter_numeric_attribute_value_expr(splitting_att, boundary_expression)
            else:
                subdata_greater_equal,subdata_less_than = data.filter_numeric_attribute_value_expr(splitting_att, boundary_expression)
            #test split entropy

            # in fact, its numeric value test
            stat_for_lt_value = len(subdata_less_than)/len(data)
            stat_for_gte_value = len(subdata_greater_equal)/len(data)

            stats=data.calculate_statistics(splitting_att)
            stats_linear=data.calculate_statistics(linear_relation_att)

            conf_for_value = (stats.get_avg_confidence()+stats_linear.get_avg_confidence())/2
            avg_abs_importance = (stats.get_avg_abs_importance()+stats_linear.get_avg_abs_importance())/2

            single_temp_gain, pure_single_temp_gain=UId3.calculate_gains_numeric(stat_for_lt_value, stat_for_gte_value, conf_for_value,avg_abs_importance,  
                                                                                 subdata_less_than,subdata_greater_equal, splitting_att, entropyEvaluator, globalEntropy, beta, shap)
            if single_temp_gain > single_temp_gain_max:
                single_temp_gain_max=single_temp_gain
                pure_single_temp_gain_max=pure_single_temp_gain
                boundary_expression_max = boundary_expression
        
        
        return single_temp_gain, pure_single_temp_gain, splitting_att, linear_relation_att, boundary_expression_max

        
    
    @staticmethod
    def try_attribute_for_split(data, attribute, cl, globalEntropy, entropyEvaluator,min_impurity_decrease, beta=1, n_jobs=None, shap=False):
        values = attribute.get_domain()
        pure_info_gain = 0
        info_gain=0
        best_split=None
        
        
        stats = data.calculate_statistics(attribute)

        ## start searching for best border values  -- such that class value remains the same for the ranges between them
        if attribute.get_type() == Attribute.TYPE_NUMERICAL:
            from sklearn.tree import DecisionTreeClassifier
            clf_h = DecisionTreeClassifier()
            tmp_df = data.to_dataframe()
            clf_h.fit(tmp_df[attribute.get_name()].values.reshape(-1,1), tmp_df[data.get_class_attribute().get_name()])
            values = np.array([clf_h.tree_.threshold[0].astype(str)])
        
            # border_search_list = []
            # for i in data.get_instances():
            #     v=i.get_reading_for_attribute(attribute).get_most_probable().get_name()
            #     border_search_list.append([v])
            # border_search_df = pd.DataFrame(border_search_list,columns=['values'])
            # border_search_df['values']=border_search_df['values'].astype('f8')
            # border_search_df['class'] = cl
            # border_search_df=border_search_df.sort_values(by='values')
            # border_search_df['values_shift']=border_search_df['values'].shift(1)
            # border_search_df['class_shitf'] = border_search_df['class'].shift(1)
            # border_search_shift = border_search_df[border_search_df['class_shitf'] != border_search_df['class']]
            # values = np.unique((border_search_shift['values']+border_search_shift['values_shift']).dropna()/2).astype('str') # take the middle value 
        else:
            values=list(values)

        if n_jobs is not None and attribute.get_type()==Attribute.TYPE_NUMERICAL: 
            if n_jobs == -1:
                n_jobs = cpu_count()
            if len(values)/n_jobs < UId3.PARALLEL_ENTRY_FACTOR:
                n_jobs = max(1,int(len(values)/UId3.PARALLEL_ENTRY_FACTOR))
        else:
            n_jobs = 1

        #divide into j_jobs batches
        if n_jobs > 1:
            values_batches = np.array_split(values, n_jobs)
            with Pool(n_jobs) as pool:
                results = pool.starmap(UId3.calculate_split_criterion, [(v, data, attribute, stats, globalEntropy, entropyEvaluator, min_impurity_decrease,beta,shap) for v in values_batches])
                temp_gain = 0
                for best_split_candidate_c, value_to_split_on_c, temp_gain_c, pure_temp_gain_c in results:
                    if temp_gain_c > temp_gain:
                        best_split_candidate=best_split_candidate_c 
                        value_to_split_on =value_to_split_on_c
                        temp_gain =temp_gain_c
                        pure_temp_gain=pure_temp_gain_c
        else:
            best_split_candidate, value_to_split_on, temp_gain, pure_temp_gain = UId3.calculate_split_criterion(values=values, 
                                                                                                                data=data, 
                                                                                                                attribute=attribute, 
                                                                                                                stats=stats, 
                                                                                                                globalEntropy=globalEntropy, 
                                                                                                             
                                                                                                                entropyEvaluator=entropyEvaluator, 
                                                                                                                min_impurity_decrease=min_impurity_decrease,
                                                                                                                beta=beta,shap=shap)


        if temp_gain > info_gain and (pure_temp_gain/globalEntropy)>=min_impurity_decrease:
            info_gain = temp_gain
            pure_info_gain=pure_temp_gain
            best_split = best_split_candidate
            best_split_candidate.set_importance_gain(pure_info_gain)
            best_split_candidate.set_value_to_split_on(value_to_split_on)
            
        return info_gain, pure_info_gain, best_split
    
    
    @staticmethod
    def calculate_gains_numeric(stat_for_lt_value, stat_for_gte_value, conf_for_value, avg_abs_importance,  subdata_less_than,subdata_greater_equal, attribute, entropyEvaluator, globalEntropy, beta,shap):
        if shap:
            #todo: WE CALCULATE RAW ENTROPY, BUT MULTIPLY STATS BY THE UNCERTAINTY
            pure_single_temp_gain = (globalEntropy - (stat_for_lt_value*entropyEvaluator.calculate_entropy(subdata_less_than)+
                                                                           (stat_for_gte_value)*entropyEvaluator.calculate_entropy(subdata_greater_equal) ))
            
            #shap_align_lt,shaptropy_lt = UId3.get_shap_stats(subdata_less_than,attribute, entropyEvaluator)
           # shap_align_gte,shaptropy_gte = UId3.get_shap_stats(subdata_greater_equal,attribute, entropyEvaluator)
            
            
#             labels_lt = [i.get_reading_for_attribute(subdata_less_than.get_class_attribute().get_name()).get_most_probable().get_name() for i in subdata_less_than.instances]
#             labels_gte = [i.get_reading_for_attribute(subdata_greater_equal.get_class_attribute().get_name()).get_most_probable().get_name() for i in subdata_greater_equal.instances]
            
#             hg = entropyEvaluator.calculate_raw_entropy(labels_lt+labels_gte)
#             hlt = entropyEvaluator.calculate_raw_entropy(labels_lt)
#             hgte = entropyEvaluator.calculate_raw_entropy(labels_gte)
            
#             prob_hlt = entropyEvaluator.calculate_entropy(subdata_less_than)
#             prob_hgte = entropyEvaluator.calculate_entropy(subdata_greater_equal)
            
#             if prob_hlt * prob_hgte == 0:
#                 pure_single_temp_gain=0
#             else:
#                 pure_single_temp_gain = (hg - (stat_for_lt_value/prob_hlt*hlt+
#                                            stat_for_gte_value/prob_hgte*hgte ))
            


            pure_single_temp_gain_shap  = avg_abs_importance*globalEntropy
            #print(f'ATT: {attribute.get_name()} Pure shap: {pure_single_temp_gain_shap}, pure gain {pure_single_temp_gain}')
            #psga =pure_single_temp_gain# (pure_single_temp_gain*beta*pure_single_temp_gain_shap)/(1+beta)
            if pure_single_temp_gain*pure_single_temp_gain_shap == 0:
                #to prevent from 0-division
                single_temp_gain=0
            else:
                single_temp_gain =(pure_single_temp_gain_shap+beta*pure_single_temp_gain)/(1+beta)#((1+beta**2)*pure_single_temp_gain_shap*pure_single_temp_gain)/((beta**2*pure_single_temp_gain_shap)+pure_single_temp_gain)*conf_for_value
        else:
            pure_single_temp_gain = (globalEntropy - (stat_for_lt_value*entropyEvaluator.calculate_entropy(subdata_less_than)+
                                                                                   (stat_for_gte_value)*entropyEvaluator.calculate_entropy(subdata_greater_equal) ))
            single_temp_gain = pure_single_temp_gain*conf_for_value
    
        return single_temp_gain, pure_single_temp_gain
    
    @staticmethod
    def get_maximum_label(shapdict):
            return max(shapdict, key=shapdict.get)
    @staticmethod    
    def get_shap_stats(data, attribute,entropyEvaluator, alignment=True):
        labels = [UId3.get_maximum_label(i.get_reading_for_attribute(attribute.get_name()).get_most_probable().get_importances()) for i in data.instances]
        if alignment:
            true_labels = [i.get_reading_for_attribute(data.get_class_attribute().get_name()).get_most_probable().get_name() for i in data.instances]
            shap_align = 0 if len(labels)== 0 else (np.array(labels)==np.array(true_labels)).sum()/len(labels)
        else:
            shap_align=1
        return shap_align,entropyEvaluator.calculate_raw_entropy(labels)


    @staticmethod
    def calculate_split_criterion( values, data, attribute, stats, globalEntropy, entropyEvaluator,min_impurity_decrease, beta=1, shap=False):
        temp_gain = 0
        temp_shapgain = 0
        temp_numeric_gain = 0
        pure_temp_gain=0
        local_info_gain = 0
        value_to_split_on = None
        best_split = None
        
        for v in values:  
            subdata = None
            subdataLessThan = None
            subdataGreaterEqual = None
                
            if attribute.get_type() == Attribute.TYPE_NOMINAL:
                subdata = data.filter_nominal_attribute_value(attribute, v)
                stat_for_value = len(subdata)/len(data)
                temp_gain += (stat_for_value) * entropyEvaluator.calculate_entropy(subdata)
            elif attribute.get_type() == Attribute.TYPE_NUMERICAL:
                subdata_less_than,subdata_greater_equal = data.filter_numeric_attribute_value(attribute, v)
                stat_for_lt_value = len(subdata_less_than)/len(data)
                stat_for_gte_value = len(subdata_greater_equal)/len(data)
                conf_for_value = stats.get_avg_confidence()
                avg_abs_importance = stats.get_avg_abs_importance()
                single_temp_gain, pure_single_temp_gain=UId3.calculate_gains_numeric(stat_for_lt_value, stat_for_gte_value, conf_for_value,  avg_abs_importance,
                                                                                     subdata_less_than,subdata_greater_equal, 
                                                                                     attribute, entropyEvaluator, globalEntropy, beta, shap)
                    
                    
                if single_temp_gain > temp_numeric_gain:
                    temp_numeric_gain = single_temp_gain
                    temp_gain = single_temp_gain
                    pure_temp_gain= pure_single_temp_gain
                    value_to_split_on = v
                    
        if attribute.get_type() == Attribute.TYPE_NOMINAL:
            conf_for_value = stats.get_avg_confidence()
            pure_temp_gain=globalEntropy-temp_gain
            if shap:
                avg_abs_importance = stats.get_avg_abs_importance()
                pure_temp_gain_shap = avg_abs_importance*globalEntropy
                temp_gain = (pure_temp_gain_shap+beta*pure_temp_gain)/(1+beta)##((1+beta**2)*pure_temp_gain_shap*pure_temp_gain)/((beta**2*pure_temp_gain_shap)+pure_temp_gain)*conf_for_value
            else:
                temp_gain = conf_for_value*pure_temp_gain

        if temp_gain > local_info_gain and (pure_temp_gain/globalEntropy)>=min_impurity_decrease:
            best_split = attribute
            local_info_gain=temp_gain
 
        return best_split, value_to_split_on, temp_gain, pure_temp_gain

    @staticmethod
    def fit_uncertain_nominal() -> None:
        data = Data.parse_uarff("../resources/machine.nominal.uncertain.arff")
        test = Data.parse_uarff("../resources/machine.nominal.uncertain.arff")

        t = UId3.fit(data, UncertainEntropyEvaluator(), 0)
        br = TreeEvaluator.train_and_test(t, test)

        print("###############################################################")
        print(f"Correctly classified instances: {br.get_accuracy() * 100}%")
        print(f"Incorrectly classified instances: {(1-br.get_accuracy()) * 100}%")
        print("TP Rate", "FP Rate", "Precision", "Recall", "F-Measure", "ROC Area", "Class")

        for class_label in data.get_class_attribute().get_domain():
            cs = br.get_stats_for_label(class_label)
            print(cs.get_TP_rate(), cs.get_FP_rate(), cs.get_precision(), cs.get_recall(), cs.get_F_measure(),
                                cs.get_ROC_area(br), cs.get_class_label())

    def predict(self, X):   # should take array-like X -> predict(X)
        if not isinstance(X, (list, np.ndarray)):
            X = [X]
        predictions = []
        for instance in X:
            att_stats = self.tree.predict(instance)
            predictions.append(att_stats.get_most_probable())
        return predictions